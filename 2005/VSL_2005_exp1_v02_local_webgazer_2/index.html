<!DOCTYPE html>
<html>

<head>
	<title>VSL Experiment</title>


	<script src="jspsych/jspsych.js"></script>
	<script src="jspsych/plugin-html-keyboard-response.js"></script>

	<script src="jspsych/plugin-html-button-response.js"></script>
	<script src="jspsych/plugin-survey-html-form.js"></script>
	<script src="jspsych/plugin-survey-text.js"></script>

	<script src="jspsych-vsl-grid-attention-check.js"></script>
	<script src="jspsych-vsl-grid-scene_snap.js"></script>
	<script src="snap.svg.js"></script>
	<script src="detect.js"></script>


	<script src="jspsych/plugin-fullscreen.js"></script>
	<link href="jspsych/jspsych.css" rel="stylesheet" type="text/css">
	</link>
	<script type="text/javascript" src="familiarization_2005e1_v02.js"></script>
	<script type="text/javascript" src="test_1.js"></script>


	<script type="text/javascript" src="jspsych-7-pavlovia-2022.1.1.js"></script>

	<script type="text/javascript" src="webgazer.js"></script> <!-- WG: loads a local instance of WebGazer -->
</head>

<body></body>
<script>

	// type="text/javascript" src="lib/vendors/jquery-2.2.0.min.js">

	//WG: handles the proper association of WG with a given webcam
	async function selectWebcam(deviceId) {
		try {
			// Stop WebGazer before switching cameras
			webgazer.pause();
			console.log("WebGazer paused before selecting new webcam.");

			// Stop any existing video stream
			let oldStream = webgazer.videoElementTrack ? webgazer.videoElementTrack : null;
			if (oldStream) {
				oldStream.stop();
				console.log("Stopped old video stream.");
			}

			// Get the new webcam stream
			const stream = await navigator.mediaDevices.getUserMedia({
				video: { deviceId: { exact: deviceId } }
			});

			let video = document.getElementById('webgazerVideoFeed');
			if (!video) {
				console.error("Video element #webgazerVideoFeed not found!");
				return;
			}

			video.srcObject = stream;

			// Wait for the video metadata to load properly
			video.onloadedmetadata = () => {
				video.play();
				console.log("Video feed started.");

				// Assign the new video track to WebGazer
				webgazer.videoElementTrack = stream.getTracks()[0];

				// Restart WebGazer
				webgazer.resume();

				console.log("WebGazer resumed with new webcam:", deviceId);
			};

			// Error handling for WebGazer texture size issue
			setTimeout(() => {
				if (video.videoWidth === 0 || video.videoHeight === 0) {
					console.warn("WebGazer might have encountered a texture size issue. Restarting...");
					restartWebGazer(video);
				}
			}, 1000); // Check after 1 second to confirm feed is working

		} catch (err) {
			console.error("Error accessing webcam:", err);
		}
	}

	// WG: restarts WG in case of errors
	function restartWebGazer(videoElement) { //WG
		console.log("Restarting WebGazer to resolve texture size issue...");

		webgazer.pause();

		// Give it a moment to release resources
		setTimeout(() => {
			webgazer.clearData(); // Reset WebGazer tracking data
			videoElement.play();  // Ensure video feed restarts
			webgazer.resume();
			console.log("WebGazer restarted successfully.");
		}, 500);
	}

	// WG: gets a list of webcams
	function getVideoInputs() { //WG
		return navigator.mediaDevices.enumerateDevices()
			.then(devices => devices.filter(device => device.kind === "videoinput"))
			.catch(error => {
				console.error("Error accessing media devices:", error);
				return [];
			});
	}

	// WG: initialization
	webgazer.setRegression('ridge')
		.setTracker('TFFacemesh')
		.showVideo(true) // always set to true, as the associated DOM element is needed for prediction state checking (see: getWebgazerTrackingState function); select_webcam_trial's on_finish hides it while keeping the element in DOM
		.showPredictionPoints(false) //set to false for deployment, true for development
		.begin();

	/* VSL 1
	jsPSych implementation of Experiment 1 from: Fiser, J., & Aslin, R. N. (2001). Unsupervised Statistical Learning of Higher-Order Spatial Structures from Visual Scenes. Psychological Science, 12(6), 499-504. doi: 10.1111/1467-9280.00392
	
	Phase 1 - Familiarization: presents stream of 6 element scenes.
	Uses scene structures stored in an array in familiarization.js. This is based on the original Matlab file, therefore indexing starts from 1 there. Fixes for that are foung throughout the script. Phase 1 is followed by a 2 minutes break.
	
	Phase 2 - 2-alternative forced choice task
	Uses test scene strucutre stored in an array in test_1.js. Same indexing issue and fix as Phase 1.
	*/

	// set paramters for all VSL scenes (familiarization and test)
	var global_image_size = 150; // 200;
	var global_line_px = 3;
	var global_post_trial_gap = 1000;
	var user_browser = (detect.parse(navigator.userAgent)).browser.family;



	// initialize experiment
	var jsPsych = initJsPsych({
		timeline: timeline,
		preload_images: image_switchboard,
		override_safe_mode: true,
		on_finish: function () {
			document.body.innerHTML = '<p style="text-align: justify;"> You will be redirected back to Prolific in a few moments. Please wait. </p>'
			setTimeout(function () { location.href = "https://app.prolific.co/submissions/complete?cc=777B2804" }, 10000)
			jsPsych.data.get().addToLast({
				randomID: randID,
				subject: participant,
				image_switchboard: image_switchboard,
				scene_order: scrambled_scene_array
			});
			// WG: exports merged jsPsych and gaze data with headers so that only a single data file is produced
			saveData("experiment_VSL_randID" + randID + ".csv", mergeCSV(jsPsych.data.get().csv(), convertArrayToCSV(gazeBuffer)));
		}
	});

	var timeline = []


	// get info about participants from prolific
	var participant = jsPsych.data.getURLVariable('participant');
	var session = jsPsych.data.getURLVariable('session');

	/* init connection with pavlovia.org */
	/* 	var pavlovia_init = {
			type: "pavlovia",
			command: "init"
		};
		timeline.push(pavlovia_init); */


	var redirect_browser = {
		type: jsPsychHtmlButtonResponse,
		stimulus: '<p style="text-align: justify;margin-left: 15%; margin-right:15%"> Unfortunately, your browser is not supported. ' +
			' Please use Google Chrome, Safari or Opera to run this experiment. ' +
			' If you have any questions about this, please email the experimenter at szabo_beata-tunde@phd.ceu.edu ' +
			' You can now close this window/tab </p>',
		choices: []
	};

	var redirect_mobile = {
		type: jsPsychHtmlButtonResponse,
		stimulus: '<p style="text-align: justify;margin-left: 15%; margin-right:15%"> Unfortunatly, this study cannot be completed on mobile devices. ' +
			' Please use a desktop computer or laptop. ' +
			' If you have any questions about this, please email the experimenter at szabo_beata-tunde@phd.ceu.edu ' +
			' You can now close this window/tab </p>',
		choices: []
	};


	// exlcude participant using edge or internet explorer
	//if (user_browser == "Firefox") {
	// pass
	//}
	if (user_browser == "Chrome") {
		// pass
	} else if (user_browser == "Safari") {
		// pass
	} else if (user_browser == "Opera") {
		// pass
	} else {
		timeline.push(redirect_browser);
	};


	if (/Mobi|Android/i.test(navigator.userAgent)) {
		timeline.push(redirect_mobile);
	}


	// create and save unique id for the participant for the Testable endscreen to see if they finished the experiment 
	var randID = jsPsych.randomization.randomID(5)


	// WG: global variable to store viewport dimensions
	let viewportDims = { width: null, height: null };

	// enter fullscreen mode
	timeline.push({
		type: jsPsychFullscreen,
		fullscreen_mode: true,
		post_trial_gap: 1,
		on_finish: function () {
			// Store viewport dimensions after entering fullscreen
			viewportDims.width = window.innerWidth;
			viewportDims.height = window.innerHeight;
			gazeTarget.x = viewportDims.width * 0.5;
			gazeTarget.y = viewportDims.height * 0.5;
			jsPsych.data.addProperties({
				viewport_dims: viewportDims
			});

			console.log(`Viewport dimensions stored: ${viewportDims.width}x${viewportDims.height}`);
		}
	});

	// WG: sets the Confirm button enabled or disabled at the webcam selector trial based face detection success
	async function checkFaceDetection() {
		const confirmButton = document.querySelector('#confirm-button');
		if (!confirmButton) return; // Ensure the button exists

		try {
			const prediction = await webgazer.getCurrentPrediction(); // Await the async function

			// Enable the confirm button only if a valid face prediction exists
			confirmButton.disabled = !prediction;

			if (prediction) {
				console.log("Face detected:", prediction);
			} else {
				console.log("No face detected.");
			}
		} catch (error) {
			console.error("Error checking WebGazer prediction:", error);
		}
	}


	// WG: select the webcam used for gaze tracking
	var select_webcam_trial = {
		type: jsPsychHtmlButtonResponse,
		stimulus: `
        <p style="text-align: center;">Please select the webcam associated with the screen you are using to view the experiment.</p>
        <p style="text-align: center;">Select your webcam:</p>
        <select id="webcam-select"></select>
    	`,
		choices: ['Confirm'],
		button_html: '<button id="confirm-button" disabled>%choice%</button>', // Disable button initially
		on_load: async function () {
			try {
				const devices = await navigator.mediaDevices.enumerateDevices();
				const videoInputs = devices.filter(device => device.kind === "videoinput");

				const select = document.querySelector('#webcam-select');
				const confirmButton = document.querySelector('#confirm-button');

				if (videoInputs.length === 0) {
					select.innerHTML = '<option>No webcams found</option>';
				} else {
					videoInputs.forEach(device => {
						const option = document.createElement('option');
						option.value = device.deviceId;
						option.textContent = device.label || 'Unknown Device';
						select.appendChild(option);
					});

					// Call function on initial selection (first option in list)
					selectWebcam(select.value);
					checkFaceDetection();
				}

				// Add onchange event listener to call function when selection changes
				select.addEventListener('change', function () {
					selectWebcam(this.value);
					checkFaceDetection();
				});
			} catch (error) {
				console.error("Error accessing media devices:", error);
			}
		},
		on_finish: function () {
			document.getElementById("webgazerVideoContainer").style.display = "none"; // set to "block" or comment out line to keep the video feedback inset visible
			startGazeCollection(); // starts collection of gaze data to the gazeBuffer array
		}
	};
	timeline.push(select_webcam_trial);

	// WG: gaze collection parameters
	let gazeOffThreshold = 0.1; //ratio of the screen ('viewport') considered to be already outside of the target area, if set to 0: whole screen is the target, if set to 0.5: only gaze at the very center pixel of the screen is considered to be on
	let gazeCollectionSampleTime = 500; //in ms

	// WG: gaze collection variables
	let gazeTarget = { x: null, y: null } //misc variable used in validation trials, mostly set to the center of the screen
	let gazeBuffer = []; // Global array to store gaze predictions
	let gazeCollectionInterval; // Variable to hold the interval ID, not the sampling time!

	// WG: gaze collection
	const startGazeCollection = async (intervalMs = gazeCollectionSampleTime) => {
		// Recursive function to collect gaze predictions at regular intervals
		console.log("Started collecting gaze data...")
		const collectGaze = async () => {
			const gazePrediction = await webgazer.getCurrentPrediction();
			if (gazePrediction) {
				gazeBuffer.push({
					timestamp: Date.now() - jsPsych.getStartTime(), // gaze collection time relative to jsPsych start time -- same time coordinates as in jsPsych
					x: gazePrediction.x,
					y: gazePrediction.y,
					xOff: !(viewportDims.width * gazeOffThreshold < gazePrediction.x && gazePrediction.x < viewportDims.width * (1 - gazeOffThreshold)),
					yOff: !(viewportDims.height * gazeOffThreshold < gazePrediction.y && gazePrediction.y < viewportDims.height * (1 - gazeOffThreshold)),
					tracking_state: getWebgazerTrackingState(),
					targetX: gazeTarget.x,
					targetY: gazeTarget.y
				});
			}
			// Schedule the next call after the interval
			gazeCollectionInterval = setTimeout(collectGaze, intervalMs);
		};

		// Start the collection
		collectGaze();
	};

	const stopGazeCollection = () => {
		// Clear the timeout to stop the recursive collection
		console.log("Stopped collecting gaze data...")
		if (gazeCollectionInterval) {
			clearTimeout(gazeCollectionInterval);
			gazeCollectionInterval = null;
		}
	};

	// WG: check tracking state, returns "good" if the face is fully visible, "problematic" if the face is only partially tracked and "face_lost" if WG cannot detect it at all
	function getWebgazerTrackingState() {
		const feedbackBox = document.getElementById("webgazerFaceFeedbackBox");

		if (!feedbackBox) {
			console.warn("WebGazer feedback box not found.");
			return "unknown";
		}

		const borderColor = window.getComputedStyle(feedbackBox).borderColor;

		if (borderColor === "rgb(0, 128, 0)") return "good";         // Green
		if (borderColor === "rgb(255, 0, 0)") return "problematic";  // Red
		if (borderColor === "rgb(0, 0, 0)") return "face_lost";      // Black

		return "unknown"; // If color doesn't match expected values
	};

	// WG: calibration parameters
	var n_clicks_required = 5;  // Number of clicks required per dot
	var dot_positions = [
		{ x: 20, y: 20 }, { x: 50, y: 20 }, { x: 80, y: 20 },
		{ x: 20, y: 50 }, { x: 50, y: 50 }, { x: 80, y: 50 },
		{ x: 20, y: 80 }, { x: 50, y: 80 }, { x: 80, y: 80 }
	];  // Positions as percentages of the viewport width/height

	// WG: instructions trial for WG calibration
	var start_calibration_trial = {
		type: jsPsychHtmlButtonResponse,
		stimulus: `
					<p>In the next step, you will see red dots appear on the screen.</p>
					<p>Your task is to click on each dot until it disappears.</p>
					<p>This process helps calibrate the system.</p>
					<p>Click the button below to begin.</p>
    `,
		choices: ['Start Calibration']
	};
	timeline.push(start_calibration_trial);


	// WG: calibration trial definition
	var click_counts = {};
	const calibration_dot_trial = {
		type: jsPsychHtmlKeyboardResponse,
		stimulus: () => {
			let dotsHtml = '';
			dot_positions.forEach((dot, index) => {
				dotsHtml += `<div id="dot-${index}" style="
														position: absolute; 
														top: ${dot.y}%; 
														left: ${dot.x}%; 
														width: 60px; 
														height: 60px; 
														background-color: red; 
														border-radius: 50%; 
														transform: translate(-50%, -50%); 
														cursor: pointer;"></div>`;
				click_counts[`dot-${index}`] = 0; // Initialize click count for each dot
			});
			return dotsHtml;
		},
		choices: 'NO_KEYS',
		on_load: () => {
			dot_positions.forEach((dot, index) => {
				const dotElement = document.getElementById(`dot-${index}`);
				const initialSize = 50;

				// Add click event listener to each dot
				dotElement.addEventListener('click', (event) => {
					const x = event.clientX;
					const y = event.clientY;

					// Record the actual mouse position
					webgazer.recordScreenPosition(x, y);

					gazeTarget = { x: (dot.x / 100) * viewportDims.width, y: (dot.y / 100) * viewportDims.height };

					// Increment click count
					click_counts[`dot-${index}`]++;
					console.log(`Dot ${index + 1} clicked ${click_counts[`dot-${index}`]}/${n_clicks_required}`);

					// Shrink the dot
					const newSize = 10 + initialSize * (1 - click_counts[`dot-${index}`] / n_clicks_required);
					dotElement.style.width = `${newSize}px`;
					dotElement.style.height = `${newSize}px`;

					// Remove dot after required clicks
					if (click_counts[`dot-${index}`] >= n_clicks_required) {
						dotElement.remove();
						delete click_counts[`dot-${index}`]; // Remove from tracking

						// Check if all dots have been clicked
						if (Object.keys(click_counts).length === 0) {
							setTimeout(() => {
								jsPsych.finishTrial();
							}, 500);
						}
					}
				});
			});
		},
		on_finish: () => {
			click_counts = {}; // Reset click counts for the next calibration trial
		},
	};
	timeline.push(calibration_dot_trial);

	// WG: instructions trial for WG validation
	var start_validation_trial = {
		type: jsPsychHtmlButtonResponse,
		stimulus: `
					<p>In the next step, you will see green dots appear one at a time on the screen.</p>
					<p>Your task is to fixate on each dot and keep looking at it until it disappears.</p>
					<p>This process helps validate the system.</p>
					<p>Click the button below to begin.</p>
    `,
		choices: ['Start Validation']
	};
	timeline.push(start_validation_trial);

	// WG: validation parameters
	const n = 5; // number of seconds each dot is shown
	dot_positions = [
		{ x: 20, y: 20 }, { x: 50, y: 20 }, { x: 80, y: 20 },
		{ x: 20, y: 50 }, { x: 50, y: 50 }, { x: 80, y: 50 },
		{ x: 20, y: 80 }, { x: 50, y: 80 }, { x: 80, y: 80 }
	];

	// WG: utility function used in validation
	function shuffleArray(array) {
		for (let i = array.length - 1; i > 0; i--) {
			const j = Math.floor(Math.random() * (i + 1));
			[array[i], array[j]] = [array[j], array[i]];
		}
		return array;
	}
	dot_positions = shuffleArray(dot_positions);

	// WG: validation trial definition
	const validation_dot_trial = {
		type: jsPsychHtmlKeyboardResponse,
		stimulus: `<div id="validation-container" style="position: relative; width: 100vw; height: 100vh;"></div>`,
		choices: "NO_KEYS",
		on_load: function () {
			let currentIndex = 0;
			const container = document.getElementById("validation-container");

			// Function to display the next dot in sequence
			function showNextDot() {
				// If we've shown all dots, finish the trial
				if (currentIndex >= dot_positions.length) {
					jsPsych.finishTrial();
					return;
				}

				// Get the current dot's position
				const dot = dot_positions[currentIndex];

				gazeTarget = { x: (dot.x / 100) * viewportDims.width, y: (dot.y / 100) * viewportDims.height };

				// Insert a dot into the container using the provided CSS class,
				// and position it using percentage values for top and left.
				container.innerHTML = `<div class="validation-dot" style="
																	position: absolute; 
																	top: ${dot.y}%; 
																	left: ${dot.x}%; 
																	width: 60px; 
																	height: 60px; 
																	background-color: yellowgreen; 
																	border-radius: 50%; 
																	transform: translate(-50%, -50%); 
																	cursor: pointer;"></div>`;
				currentIndex++;

				// Schedule the next dot after n seconds
				setTimeout(showNextDot, n * 1000);
			}

			// Start the dot sequence
			showNextDot();
		},
		on_finish: function () {
			console.log("Validation dot trial finished.");
		}
	};
	timeline.push(validation_dot_trial);

	var welcome_trial = {
		type: jsPsychHtmlButtonResponse,
		stimulus: '<p style="text-align: justify;margin-left: 15%; margin-right:15%">You are participating in an experiment at the Vision Lab of the Department of Cognitive Science, Central European University. ' +
			' One of our main questions is what kind of representations we can create about our visual environment and how these representations influence our behavior. ' +
			'At our lab we use multiple methods, for example behavioral experiments and computational models. ' +
			'In this experiment you will observe scenes consisting of abstract shapes and answer simple questions about them. <br><br>' +
			'At the beginning and throughout the experiment you will receive precise information about what to do. The experiment takes about 20 minutes in total. <br><br>' +
			'The test is voluntary, and you can withdraw your participation at any time. We treat your personal information confidentially. <br>' +
			'Only those people who work on this research project have access to your data. We will work with group level statistics. We will only report the results of the group in scientific publications and conferences. ' +
			'This research was approved by the Hungarian Psychological Research Ethics Committee.<br><br>' +
			'If you have any further questions about the study, you can contact the experimenter, Beata Tunde Szabo, PhD Student. Contact: 1051 Nador u. 11. E-mail: szabo_beata-tunde@phd.ceu.edu <br><br>' +
			'By pressing the button below you indicate that you read this information, found it sufficent, and you consent to participate in the experiment. <br><br> </p>',
		choices: ['I consent to participate in the experiment']
	};
	timeline.push(welcome_trial)


	var survey = {
		type: jsPsychSurveyHtmlForm,
		preamble: 'Before you start the experiment, please answer these non-identifying demographic questions. <br> These questions are not a screener. <br><br><br>',
		html: '<p><label for="Age">Age:</label> <input name="Age" id="Age" type="text" required/>  <br><br> </p>' +
			'<p> Gender:</label> </p>' +
			'<div> <input type="radio" id="Female" name="gender" value="Female" checked> <label for="Female">Female</label>' +
			'&emsp;<input type="radio" id="Male" name="gender" value="Male"><label for="Male">Male</label>' +
			'&emsp;<input type="radio" id="Other" name="gender" value="Other"><label for="Other"></label>' +
			'<input type="text" name="gender_other" /> </div> <br><br>',
		button_label: ['Continue to the instructions']
	};
	//  timeline.push(survey)


	var instructions_fam = {
		type: jsPsychHtmlButtonResponse,
		stimulus: '<p style="text-align: justify;margin-left: 15%; margin-right:15%"> This is your practice session. You will see a checkerboard with random simple shapes ' +
			'appearing in different positions in the checkerboard. Your task is simply to observe ' +
			'the patterns so that you would be able to answer some questions about the sequence ' +
			'after the practice session.  The session will last for about 8 minutes.  Please, pay ' +
			'attention to EACH display! ' +
			'<br><br>' +
			'Occasionally, there will be an attention check asking you to press the spacebar. ',
		choices: ['Start the experiment.']
	}
	timeline.push(instructions_fam)


	// Attention Check - START /////////////////////////////////////////////////
	var positions = [
		[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],
		[1, 0], [1, 1], [1, 2], [1, 3], [1, 4],
		[2, 0], [2, 1], [2, 3], [2, 4],
		[3, 0], [3, 1], [3, 2], [3, 3], [3, 4],
		[4, 0], [4, 1], [4, 2], [4, 3], [4, 4]
	];

	// creats and displays a random attention check scene
	var attention_trial = {
		type: jsPsychVslGridAttentionCheck,
		stimuli: function () {
			var this_att_check = [
				[0, 0, 0, 0, 0],
				[0, 0, 0, 0, 0],
				[0, 0, "img/attention.png", 0, 0],
				[0, 0, 0, 0, 0],
				[0, 0, 0, 0, 0]
			];
			var this_postions = jsPsych.randomization.sampleWithoutReplacement(positions, 5);
			for (let l = 0; l < this_postions.length; l++) {
				this_att_check[this_postions[l][0]][this_postions[l][1]] = "img/square.png"
			}
			return this_att_check;
		},
		image_size: [global_image_size, global_image_size],
		line_px: global_line_px,
		trial_duration: 3000,
		post_trial_gap: 0,
		choices: [' '],
		stimulus_duration: 2000
	};

	// loops attention check scenes until spacbar was pressed
	var attention_loop = {
		timeline: [attention_trial],
		loop_function: function (data) {
			if (jsPsych.pluginAPI.compareKeys(' ', data.values()[0].response)) {
				return false;
			} else {
				return true;
			}
		}
	};

	// add 1 second blank screen as iti between pressing of space bar and next VSL scene
	var iti_trial = {
		type: jsPsychHtmlKeyboardResponse,
		choices: "NO_KEYS",
		stimulus: ' ',
		trial_duration: 1000,
	};

	// this needs to be pushed to the timeline to implement a full attention check
	var attention_check = {
		timeline: [attention_loop, iti_trial]
	};
	// Attention Check - END  //////////////////////////////////////////////////


	// PAHSE 1 - Familiarization   START ////////////////////////////////////////
	var scene_array = [];

	var fam_scenes = familiarization_scenes; // array from familiarization.js file

	//for debugging
	// var fam_scenes = [
	// 	[1,1,1,2,1,3,3,2,5],
	// 	[1,1,2,2,1,3,3,2,5],
	// 	[1,1,1,2,1,4,3,2,5]
	// ]

	var images = ["img/obj1.png", "img/obj2.png", "img/obj3.png", "img/obj4.png", "img/obj5.png", "img/obj6.png", "img/obj7.png", "img/obj8.png", "img/obj9.png", "img/obj10.png", "img/obj11.png", "img/obj12.png"]

	var image_switchboard = jsPsych.randomization.repeat(images, 1);

	var word_decode = [
		[], // because the original file indexes from 1 not 0
		[0, 1, 1, 1, 2, 2, 2],  // shape ID is minus 1 compared to Matlab version because of indexing
		[3, 4, 1, -1, 5, 2, -2],
		[6, 7, 1, 1, 8, 0, 2],
		[9, 10, -1, 1, 11, 0, 2]
	];

	var scene = [] // I don not need this anymore, for experimental control maybe for storing?

	for (let i = 0; i < fam_scenes.length; i++) { //fam_scenes.length; i++) {//SHORTEN fam_scenes.length; i++) {
		var this_scene = [
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0]
		];

		this_word = word_decode[fam_scenes[i][2]]; // get first word
		this_scene[fam_scenes[i][1]][fam_scenes[i][0]] = image_switchboard[this_word[0]] // place first letter of first word on its position
		this_scene[fam_scenes[i][1] + this_word[3]][fam_scenes[i][0] + this_word[2]] = image_switchboard[this_word[1]] // place second letter of word in its position
		this_scene[fam_scenes[i][1] + this_word[6]][fam_scenes[i][0] + this_word[5]] = image_switchboard[this_word[4]] // place third letter of word in its position


		this_word = word_decode[fam_scenes[i][5]]; // get second word
		this_scene[fam_scenes[i][4]][fam_scenes[i][3]] = image_switchboard[this_word[0]] // place first letter of second word on its position
		this_scene[fam_scenes[i][4] + this_word[3]][fam_scenes[i][3] + this_word[2]] = image_switchboard[this_word[1]] // place second letter of word in its position
		this_scene[fam_scenes[i][4] + this_word[6]][fam_scenes[i][3] + this_word[5]] = image_switchboard[this_word[4]] // place second letter of word in its position

		scene.push(this_scene) // I don not need this anymore, for experimental control maybe for storing?

		var this_trial = {
			type: jsPsychVslGridSceneSnap,
			stimuli: this_scene,
			image_size: [global_image_size, global_image_size],
			line_px: global_line_px,
			post_trial_gap: global_post_trial_gap
		}
		scene_array.push(this_trial)
	};

	var scrambled_scene_array = jsPsych.randomization.repeat(scene_array, 1);


	var fam_part_1 = {
		timeline: scrambled_scene_array.slice(0, 48)
	};
	var fam_part_2 = {
		timeline: scrambled_scene_array.slice(48, 96)
	};
	var fam_part_3 = {
		timeline: scrambled_scene_array.slice(96)
	};


	//timeline holding all scenes in randomized order and attention 2 attention checks
	var VSL_trials_procedure = {
		timeline: [fam_part_1, attention_check, fam_part_2, attention_check, fam_part_3, attention_check, fam_part_1, attention_check, fam_part_2, attention_check, fam_part_3]
	};
	// var VSL_trials_procedure = {
	// 	timeline: [fam_part_1, attention_check, fam_part_2, attention_check, fam_part_3]
	// };
	timeline.push(VSL_trials_procedure);

	// PAHSE 1 - Familiarization   END


	var countdown_procedure = {
		timeline: []
	};

	for (let counter = 0; counter < 120; counter++) { //120; counter++) {//SHORTEN 120; counter++) {
		var countdown_trial = {
			type: jsPsychHtmlKeyboardResponse,
			stimulus: '<p>Your practice session is now over.</p>' +
				'<p>The test session will start in 2 minutes.<p>' +
				'<p>You can use these two minutes to relaxe your eyes.</p>' +
				'<p> <br> </p>' +
				'<p>Seconds remaining:</p>' +
				'<p> <br> </p>' +
				(120 - counter),
			choices: "NO_KEYS",
			trial_duration: 1000,
		}
		countdown_procedure.timeline.push(countdown_trial)
	};
	timeline.push(countdown_procedure);



	var instructions_test = {
		type: jsPsychHtmlKeyboardResponse,
		stimulus: '<p style="text-align: justify;margin-left: 15%; margin-right:15%"> This is your test session. You will see 36 trials. In each trial two pairs of shapes ' +
			'will appear with one second of pause between them. Your task is to decide which of the ' +
			'two pairs looks more familiar based on the practice session you have just seen. ' +
			'After the second shape-pair a dialog window will appear asking whether the first or the ' +
			'second pair was more familiar. Using the keyboard you should press 1 or 2. ' +
			'<br> Pressing 1 means that the first shape-pair is more familiar. ' +
			'<br> Pressing 2 means that the second shape-pair is more familiar. ' +
			'<br> No other inputs are accepted. ' +
			'<br> It is important that you pay attention to each trial and that you reply to each trial to the best of your knowledge. ' +
			'Always reply either 1 or 2, even if you are unsure. ' +
			'<br><br>' +
			'Press space bar to begin.<p>'
	}
	timeline.push(instructions_test)



	// PAHSE 2 - 2AFC task   START  //DEBUG START

	var test_array = []

	var test_scenes = test_scenes_1; // array from test_1.js file

	var nonword_decode = [
		[], // because the original file indexes from 1 not 0; shape ID is minus 1 compared to Matlab version because of indexing
		[1, 1, 0, 0, 1, 0, 0, 1, 0, 0], // The 6 single nonwords are all Y-s because what is said above 
		[3, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[5, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[7, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[9, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[11, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[3, 8, 1, 1, 1, 0, 0, 1, 0, 0], // Here start the 4 valuable nonword pairs: DI
		[4, 7, 1, -1, 1, 0, 0, 1, 0, 0], // EH
		[6, 5, 1, -1, 1, 0, 0, 1, 0, 0], // GF
		[3, 7, 1, -1, 1, 0, 0, 1, 0, 0], // DH
		[8, 6, 0, 1, 1, 0, 0, 1, 0, 0], // These are the two dummy pair nonwords: IG
		[11, 9, 1, 0, 1, 0, 0, 1, 0, 0], // LJ
		[10, 0, 1, 1, 11, 2, 2, 1, 0, 0], // Here start the 2 good triplet nonwords: KAL
		[1, 9, -1, 1, 2, 0, 2, 1, 0, 0], // BJC
		[4, 2, 1, 0, 8, 2, 0, 1, 0, 0], // These are the two dummy triplet nonwords: ECI
		[6, 9, 0, 1, 0, 1, 1, 1, 0, 0], // GJA
		[1, 4, 1, 0, 6, 0, 1, 11, 1, 1], // These are the 3 dummy quadruplet nonwords: square
		[11, 0, 0, 1, 9, -1, 2, 8, 1, 1], // left Y
		[3, 2, -1, 0, 7, -1, 1, 5, 1, 0] // left L
	];


	var word_decode_test = [
		[], // because the original file indexes from 1 not 0, shape ID is minus 1 compared to Matlab version because of indexing
		[0, 1, 0, 0, 1, 0, 0, 1, 0, 0], 	// The 6 singles are all X-s because the first element will be overwritten by a randomization 
		[2, 1, 0, 0, 1, 0, 0, 1, 0, 0],					// and the rest will not be shown ever. The 0 0 position change signals that they are not used.
		[4, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[6, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[8, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[10, 1, 0, 0, 1, 0, 0, 1, 0, 0],
		[0, 1, 1, 1, 1, 0, 0, 1, 0, 0],					// Here start the 4 valuable word pairs: AB
		[1, 2, 1, 1, 1, 0, 0, 1, 0, 0],					// BC
		[9, 10, -1, 1, 1, 0, 0, 1, 0, 0],				// JK
		[10, 11, 1, 1, 1, 0, 0, 1, 0, 0],				// KL
		[0, 2, 0, -1, 1, 0, 0, 1, 0, 0],				// These are the two dummy pair words: AC
		[4, 5, -1, 0, 1, 0, 0, 1, 0, 0],				// EF
		[3, 4, 1, -1, 5, 2, -2, 1, 0, 0],				// Here start the 2 good triplet words: DEF
		[6, 7, 1, 1, 8, 0, 2, 1, 0, 0], 				// GHI
		[3, 1, 0, 1, 11, 0, 2, 1, 0, 0],				// These are the two dummy triplet words:DBL
		[5, 10, 1, 0, 7, 1, 1, 1, 0, 0],				// FKH
		[8, 3, -1, 1, 0, 0, 2, 10, 1, 1],				// These are the 3 dummy quadruplet words: a ring
		[2, 7, -1, 1, 9, 0, 1, 5, 1, 2],				// right Y
		[1, 6, -1, 0, 4, 1, 0, 10, 1, 1]				// right L
	];



	// Randomize the single elements
	var permutedArray = Array.from({ length: 12 }, (_, i) => i);
	permutedArray.sort(() => Math.random() - 0.5);

	singleElementRandomize = permutedArray;
	for (icount = 1; icount <= 6; icount++) {
		word_decode_test[icount][0] = singleElementRandomize[icount - 1];
		nonword_decode[icount][0] = singleElementRandomize[6 + icount - 1];
	}
	// Randomize the quadruple elements Online select the image elements of the quads

	var permutedArray = Array.from({ length: 12 }, (_, i) => i);
	permutedArray.sort(() => Math.random() - 0.5);

	quadElementRandomize = permutedArray;
	word_decode_test[17][0] = quadElementRandomize[0];
	word_decode_test[17][1] = quadElementRandomize[1];
	word_decode_test[17][4] = quadElementRandomize[2];
	word_decode_test[17][7] = quadElementRandomize[3];
	word_decode_test[18][0] = quadElementRandomize[4];
	word_decode_test[18][1] = quadElementRandomize[5];
	word_decode_test[18][4] = quadElementRandomize[6];
	word_decode_test[18][7] = quadElementRandomize[7];
	word_decode_test[19][0] = quadElementRandomize[8];
	word_decode_test[19][1] = quadElementRandomize[9];
	word_decode_test[19][4] = quadElementRandomize[10];
	word_decode_test[19][7] = quadElementRandomize[11];

	var permutedArray = Array.from({ length: 12 }, (_, i) => i);
	permutedArray.sort(() => Math.random() - 0.5);
	quadElementRandomize = permutedArray;
	nonword_decode[17][0] = quadElementRandomize[0];
	nonword_decode[17][1] = quadElementRandomize[1];
	nonword_decode[17][4] = quadElementRandomize[2];
	nonword_decode[17][7] = quadElementRandomize[3];
	nonword_decode[18][0] = quadElementRandomize[4];
	nonword_decode[18][1] = quadElementRandomize[5];
	nonword_decode[18][4] = quadElementRandomize[6];
	nonword_decode[18][7] = quadElementRandomize[7];
	nonword_decode[19][0] = quadElementRandomize[8];
	nonword_decode[19][1] = quadElementRandomize[9];
	nonword_decode[19][4] = quadElementRandomize[10];
	nonword_decode[19][7] = quadElementRandomize[11];

	var permutedArray = Array.from({ length: test_scenes.length }, (_, i) => i);
	// commented out for debugging: permutedArray.sort(() => Math.random() - 0.5);
	presentationSwitchBoard = permutedArray


	for (let j = 0; j < test_scenes.length; j++) {//SHORTEN test_scenes.length; j++) {

		var this_test_trial = { // contains both test sceneces plus the decision
			timeline: []
		};

		var this_test_stimuli = { // contains both test scenes
			timeline: []
		};

		// create true word scene
		var first_test_scene = [
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0]
		];

		this_test_scene = test_scenes[presentationSwitchBoard[j]]
		patternsize = this_test_scene[7]
		this_word = word_decode_test[this_test_scene[2]]; // get the word

		first_test_scene[this_test_scene[1] - 1][this_test_scene[0] - 1] = image_switchboard[this_word[0]] // place first letter of the word on its position

		if (patternsize > 1) {
			first_test_scene[this_test_scene[1] + this_word[3] - 1][this_test_scene[0] + this_word[2] - 1] = image_switchboard[this_word[1]] // place second letter of the word in its position
		}
		if (patternsize > 2) {
			first_test_scene[this_test_scene[1] + this_word[6] - 1][this_test_scene[0] + this_word[5] - 1] = image_switchboard[this_word[4]] // place second letter of the word in its position
		}
		if (patternsize > 3) {
			first_test_scene[this_test_scene[1] + this_word[9] - 1][this_test_scene[0] + this_word[8] - 1] = image_switchboard[this_word[7]] // place second letter of the word in its position
		}


		var true_test_scene = {
			type: jsPsychVslGridSceneSnap,
			stimuli: first_test_scene,
			image_size: [global_image_size, global_image_size],
			line_px: global_line_px,
			post_trial_gap: global_post_trial_gap
		}

		// create foil word scene
		var second_test_scene = [
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0],
			[0, 0, 0, 0, 0]
		];

		this_word = nonword_decode[this_test_scene[5]]; // get the word
		second_test_scene[this_test_scene[4] - 1][this_test_scene[3] - 1] = image_switchboard[this_word[0]] // place first letter of the word on its position

		if (patternsize > 1) {
			second_test_scene[this_test_scene[4] + this_word[3] - 1][this_test_scene[3] + this_word[2] - 1] = image_switchboard[this_word[1]] // place second letter of the word in its position
		}
		if (patternsize > 2) {
			second_test_scene[this_test_scene[4] + this_word[6] - 1][this_test_scene[3] + this_word[5] - 1] = image_switchboard[this_word[4]] // place second letter of the word in its position
		}
		if (patternsize > 3) {
			second_test_scene[this_test_scene[4] + this_word[9] - 1][this_test_scene[3] + this_word[8] - 1] = image_switchboard[this_word[7]] // place second letter of the word in its position
		}

		var foil_test_scene = {
			type: jsPsychVslGridSceneSnap,
			stimuli: second_test_scene,
			image_size: [global_image_size, global_image_size],
			line_px: global_line_px,
			post_trial_gap: global_post_trial_gap
		}

		var pres_order = this_test_scene[6]  // order is defined in the testkeyfile, 7th column (index 6)
		var correct = 9

		// pres_order = 1 // for debug

		if (pres_order == 1) {
			this_test_stimuli.timeline.push(true_test_scene)
			this_test_stimuli.timeline.push(foil_test_scene)
			correct = '1'

		} else {
			this_test_stimuli.timeline.push(foil_test_scene)
			this_test_stimuli.timeline.push(true_test_scene)
			correct = '2'
		}

		this_test_trial.timeline.push(this_test_stimuli)

		var this_decision = {
			type: jsPsychHtmlKeyboardResponse,
			stimulus: '<p>Was the first (1) or second (2) more familiar?</p>' +
				'<p>Press key (1) or (2)<p>',
			choices: ["1", "2"],
			post_trial_gap: 1000,
			trial_duration: 2000,
			data: {
				test_part: 'test',
				correct_response: correct,
				test_number: j,
				test_scene: presentationSwitchBoard[j],
				patternsize: patternsize,
				true_test: this_test_scene[8]
			},
			on_finish: function (data) {
				data.correct = jsPsych.pluginAPI.compareKeys(data.response, data.correct_response)  // correct is true if key press is the same as correct response;
			}
		}
		this_test_trial.timeline.push(this_decision)

		test_array.push(this_test_trial)
	}

	//scrambled_test_array = jsPsych.randomization.repeat(test_array, 1);

	var test_procedure = {
		timeline: test_array
	};
	timeline.push(test_procedure)
	//DEBUG ENd

	// PAHSE 2 - 2AFC task   END


	// PHASE 3 - Open questions   START

	var post_survey_1 = {
		type: jsPsychSurveyText,
		questions: [{ prompt: " The main part of the experiment is now over. Please, explain with your own words what do you think the experiment was about. ", rows: 5, columns: 50, required: true }]
	};
	var post_survey_2 = {
		type: jsPsychSurveyText,
		questions: [{ prompt: " In the first part of the experiment: Did you notice any regularities when you observed the sequence of scenes? If yes, please describe them! ", rows: 5, columns: 50, required: true }]
	};
	var post_survey_3 = {
		type: jsPsychSurveyText,
		questions: [{ prompt: " In the first part of the experiment: Did you notice that the used shapes always appeard in triplets? ", rows: 5, columns: 50, required: true }]
	};
	var post_survey_4 = {
		type: jsPsychSurveyText,
		questions: [{ prompt: "Did you ever participate in an experiment similar to this one? If yes, please roughly describe it in one or two senteces.", rows: 5, columns: 50, required: true }]
	};

	timeline.push(post_survey_1, post_survey_2, post_survey_3, post_survey_4);


	// PHASE 3 - Open questions   END


	// exit fullscreen mode
	timeline.push({
		type: jsPsychFullscreen,
		fullscreen_mode: false
	});

	// add participant Nr to data


	var debriefing_trial = {
		type: jsPsychHtmlButtonResponse,
		stimulus: '<p style="text-align: justify; margin-right: 15%; margin-left: 15%">  This is the end of the experiment. <br><br>' +
			'Thank you for your participation! <br><br>' +
			'If you would like any additional information, please email the experimenter at szabo_beata-tunde@phd.ceu.edu <br><br>' +
			'Please press the button bellow to be redirected to Prolific to collect your credits. <br>',
		choices: ['Continue']
	};
	timeline.push(debriefing_trial)

	jsPsych.data.get().addToLast({
		randomID: randID,
		subject: participant,
		image_switchboard: image_switchboard,
		scene_order: scrambled_scene_array
	});


	// finish connection with pavlovia.org
	/* 	var pavlovia_finish = {
			type: "pavlovia",
			command: "finish",
			participantId: participant,
			session: session
			};
		timeline.push(pavlovia_finish);
	 */


	jsPsych.run(timeline);

	// WG: utility functions to merge gaze data with other jsPsych produced data
	function convertArrayToCSV(dataArray) {
		const headers = Object.keys(dataArray[0]).join(","); // Extract column headers
		const rows = dataArray.map(obj => Object.values(obj).join(",")); // Extract row data

		return [headers, ...rows].join("\n"); // Combine headers and rows
	}
	function mergeCSV(csv1, csv2, label1 = "Experiment Data", label2 = "Gaze Data") {
		return `### ${label1} ###\n${csv1}\n\n### ${label2} ###\n${csv2}`;
	}

	function saveData(name, data) {
		var csvString = data;

		var a = window.document.createElement('a');
		a.setAttribute('href', 'data:text/csv; charset=utf-8,' + encodeURIComponent(csvString));
		a.setAttribute('download', name);
		window.document.body.appendChild(a);
		a.click();
		//var xhr = new XMLHttpRequest();
		//xhr.open('POST', 'write_data.php'); // 'write_data.php' is the path to the php file described above.
		//xhr.setRequestHeader('Content-Type', 'application/json');
		//xhr.send(JSON.stringify({ filename: name, filedata: data })); -->
	}
</script>

</html>